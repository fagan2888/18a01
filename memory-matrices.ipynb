{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with Memory Access and Matrices\n",
    "\n",
    "In many problems, especially problems accessing lots of data and doing relatively simple computations on each datum, the performance bottleneck is memory rather than computational speed.  Because memory is arranged into a **memory hierarchy** of larger/slower and smaller/faster memories, it turns out that *changing the order* of memory access can have a huge impact on performance.\n",
    "\n",
    "In this notebook, we'll explore these performance issues with a few typical matrix algorithms, implemented in Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools # a useful package of benchmarking utilities\n",
    "versioninfo() # a useful function to print out information about the machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Matrix Multiplication\n",
    "\n",
    "One of the most basic building blocks of numerical linear algebra is the computation of matrix multiplication: given an $m \\times n$ matrix $A$ and an $n \\times p$ matrix $B$, compute the $m \\times p$ matrix $C = AB$.   The entries of $C$ are given by the exact formula:\n",
    "$$\n",
    "C_{ik} = \\sum_{j=1}^n A_{ij} B_{jk}\n",
    "$$\n",
    "but there are many ways to implement this computation.   $\\approx 2mnp$ flops (floating-point additions and multiplications) are required, but they can re-ordered arbitrarily, leading to $\\sim (mnp)!$ possible orderings.\n",
    "\n",
    "It turns out that the ordering of the operations in the matrix multiplication has a *huge* impact on performance, along with low-level details of the inner loops.  Basically, three factors make the implementation of efficient matrix multiplication highly nontrivial:\n",
    "\n",
    "* [Caches](https://en.wikipedia.org/wiki/CPU_cache): the matrix accesses must be reordered to obtain [temporal locality](https://en.wikipedia.org/wiki/Locality_of_reference) and hence efficient memory (cache) usage.\n",
    "* [Registers](https://en.wikipedia.org/wiki/Processor_register): at the lowest level, the CPU registers form a kind of ideal cache.  The innermost loops of the matrix multiplication need to be unrolled in order to load many values into registers and perform as much work with them as possible (essentially a small submatrix multiplication).  It turns out that a [lot of tricks](http://cscads.rice.edu/workshops/july2007/autotune-slides-07/Frigo.pdf) are required to do this well.\n",
    "* [SIMD instructions](https://en.wikipedia.org/wiki/SIMD): modern CPUs include special instructions that can perform several arithmetic operations at once (e.g. 2, 4, or even 8 `Float64` operations), and to get the full benefit of these operations typically requires hand coding. \n",
    "\n",
    "As a consequence, there is a huge performance gap between the most obvious three-loop matrix-multiplication code and highly optimized code.  This gap has become the central factor in the design of dense linear-algebra libraries for several decades, especially the industry-standard free/open-source the [LAPACK](https://en.wikipedia.org/wiki/LAPACK) library: nearly all dense linear algebra is now organized around highly optimized [BLAS](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms) libraries.\n",
    "\n",
    "Because Julia benefits from fast compilers, we can illustrate this performance gap fairly with simple Julia code.  (In contrast, similar implementation in Matlab or Python would be orders of magnitude slower, and would demonstrate mostly language rather than the algorithmic effects.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the simplest, most obvious, matrix-multiplication algorithm: just three nested loops, implementing a dot product for each output $C_{ik}$.\n",
    "\n",
    "The only concessions we have made to performance concerns here are (1) we implement an in-place `matmul!` variant that operates on a pre-existing `C` array, to avoid benchmarking the memory allocation/deallocation and (2) we use the `@inbounds` macro to turn off array bounds-checking in Julia for the inner loop.   Together, these make less than a factor of two difference in speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matmul (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute C = A * B, using naive matrix-multiplication algorithm,\n",
    "# with a pre-allocated output array C.  (\"!\" is a Julia convention\n",
    "# for functions that modify their arguments.)\n",
    "function matmul!(C, A, B)\n",
    "    m,n = size(A)\n",
    "    n,p = size(B)\n",
    "    size(C) == (m,p) || error(\"incorrect dimensions \", size(C), \" ≠ $m × $p\")\n",
    "    for i = 1:m\n",
    "        for k = 1:p\n",
    "            c = zero(eltype(C))\n",
    "            for j = 1:n\n",
    "                @inbounds c += A[i,j] * B[j,k]\n",
    "            end\n",
    "            @inbounds C[i,k] = c\n",
    "        end\n",
    "    end\n",
    "    return C\n",
    "end\n",
    "\n",
    "# a wrapper that allocates C of an appropriate type\n",
    "matmul(A, B) = matmul!(Array{promote_type(eltype(A), eltype(B))}(\n",
    "                             size(A,1), size(B,2)),\n",
    "                       A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.634357216034927e-16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correctness check:\n",
    "A = rand(5,6)\n",
    "B = rand(6,7)\n",
    "norm(matmul(A,B) - A * B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmarking naive `matmul`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will benchmark our naive `matmul` implementation against the highly optimized [OpenBLAS library](http://www.openblas.net/) that Julia uses for its built-in matrix multiplication.   Like `matmul!`, we will call OpenBLAS with pre-allocated output via `A_mul_B!(C, A, B)` instead of the simpler `A * B`.  By default, OpenBLAS uses multiple CPU cores, which gives it an \"unfair\" parallel speedup, but we can disable this for benchmarking purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for benchmarking, use only single-threaded BLAS:\n",
    "BLAS.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will benchmark $n \\times n$ matrix multiplications for various $n$ from 10 to 1000.  Julia's `@elapsed ...code...` macro is useful for benchmarking: it times the code and returns the time in seconds.   As we go, we will print the ratio of the naive time to the optimized time, to see the slowdown of our naive code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished n = 10: slowdown of 0.003439432651965679\n",
      "finished n = 11: slowdown of 0.2545180722891566\n",
      "finished n = 12: slowdown of 0.873394495412844\n",
      "finished n = 13: slowdown of 0.9436201780415431\n",
      "finished n = 15: slowdown of 1.2254395036194417\n",
      "finished n = 16: slowdown of 1.7104773713577186\n",
      "finished n = 18: slowdown of 1.9799176107106071\n",
      "finished n = 20: slowdown of 2.508458192363461\n",
      "finished n = 22: slowdown of 2.62043795620438\n",
      "finished n = 24: slowdown of 0.5429614993114185\n",
      "finished n = 26: slowdown of 3.1448810478481692\n",
      "finished n = 29: slowdown of 3.594606164383561\n",
      "finished n = 32: slowdown of 5.246299977910316\n",
      "finished n = 35: slowdown of 3.394547759932375\n",
      "finished n = 39: slowdown of 5.196760362077179\n",
      "finished n = 43: slowdown of 3.9040229119488474\n",
      "finished n = 47: slowdown of 5.2670193034279045\n",
      "finished n = 52: slowdown of 7.325154234436344\n",
      "finished n = 57: slowdown of 7.124171499585749\n",
      "finished n = 63: slowdown of 7.51951927730279\n",
      "finished n = 69: slowdown of 7.154025331198136\n",
      "finished n = 76: slowdown of 10.848463210183173\n",
      "finished n = 84: slowdown of 12.168345969672787\n",
      "finished n = 92: slowdown of 9.894467228971601\n",
      "finished n = 102: slowdown of 10.929497196433495\n",
      "finished n = 112: slowdown of 13.0723730346671\n",
      "finished n = 123: slowdown of 12.980812000895588\n",
      "finished n = 136: slowdown of 15.590789142595378\n",
      "finished n = 150: slowdown of 16.17888519603147\n",
      "finished n = 165: slowdown of 17.58217862267965\n",
      "finished n = 182: slowdown of 18.747335209088245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mround(::Type{T}, x::AbstractArray) where T is deprecated, use round.(T, x) instead.\u001b[39m\n",
      "Stacktrace:\n",
      " [1] \u001b[1mdepwarn\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::Symbol\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./deprecated.jl:70\u001b[22m\u001b[22m\n",
      " [2] \u001b[1mround\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Type{Int64}, ::Array{Float64,1}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./deprecated.jl:57\u001b[22m\u001b[22m\n",
      " [3] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:522\u001b[22m\u001b[22m\n",
      " [4] \u001b[1mexecute_request\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::ZMQ.Socket, ::IJulia.Msg\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/stevenj/.julia/v0.6/IJulia/src/execute_request.jl:193\u001b[22m\u001b[22m\n",
      " [5] \u001b[1m(::Compat.#inner#14{Array{Any,1},IJulia.#execute_request,Tuple{ZMQ.Socket,IJulia.Msg}})\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/stevenj/.julia/v0.6/Compat/src/Compat.jl:332\u001b[22m\u001b[22m\n",
      " [6] \u001b[1meventloop\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::ZMQ.Socket\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/stevenj/.julia/v0.6/IJulia/src/eventloop.jl:8\u001b[22m\u001b[22m\n",
      " [7] \u001b[1m(::IJulia.##13#16)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./task.jl:335\u001b[22m\u001b[22m\n",
      "while loading In[5], in expression starting on line 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished n = 200: slowdown of 20.744656162817975\n",
      "finished n = 221: slowdown of 20.409211484147644\n",
      "finished n = 243: slowdown of 19.86331608581282\n",
      "finished n = 268: slowdown of 19.133545394614316\n",
      "finished n = 295: slowdown of 22.49223626607818\n",
      "finished n = 325: slowdown of 17.95879783625162\n",
      "finished n = 358: slowdown of 24.751094820215997\n",
      "finished n = 394: slowdown of 27.12657766258112\n",
      "finished n = 434: slowdown of 26.018988143044584\n",
      "finished n = 478: slowdown of 25.700658917753287\n",
      "finished n = 526: slowdown of 20.302710352017133\n",
      "finished n = 580: slowdown of 28.772426389152816\n",
      "finished n = 639: slowdown of 26.56611380542917\n",
      "finished n = 704: slowdown of 30.649836279537535\n",
      "finished n = 775: slowdown of 27.86497068494215\n",
      "finished n = 854: slowdown of 29.046256303689397\n",
      "finished n = 940: slowdown of 28.429680508365756\n",
      "finished n = 1036: slowdown of 25.603745960821687\n",
      "finished n = 1141: slowdown of 41.45394569890129\n",
      "finished n = 1257: slowdown of 34.7506882703886\n",
      "finished n = 1384: slowdown of 45.97549365476827\n",
      "finished n = 1525: slowdown of "
     ]
    }
   ],
   "source": [
    "N = round.(Int, logspace(1, log10(3000), 60))  # 60 sizes from 10 to 3000\n",
    "# alternatively, use N = 10:1000 to see some interesting patterns due to cache associativity etc.\n",
    "t = Float64[]\n",
    "t0 = Float64[]\n",
    "for n in N\n",
    "    A = zeros(n,n)\n",
    "    B = zeros(n,n)\n",
    "    # preallocate output C so that allocation is not included in timing\n",
    "    C = zeros(n,n)\n",
    "    push!(t, @elapsed matmul!(C,A,B))\n",
    "    push!(t0, @elapsed A_mul_B!(C,A,B))\n",
    "    println(\"finished n = $n: slowdown of \", t[end]/t0[end])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot # a plotting library based on Python's Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will plot the results.  Since the number of flops is $2n^3$, we will plot $2n^3 / t$ for time $t$ in microseconds in order to plot the **gigaflops rate** (billions of flops per second).  If you naively think of a CPU as a box that performs floating-point instructions at a fixed rate, with all other instructions being negligible (a picture that *may* have been true circa 1985), this would be a flat horizontal line independent of $n$, but we will see that reality is quite different.\n",
    "\n",
    "The OpenBLAS library gets an \"unfair\" factor of 8 speedup on typical modern Intel processors thanks to hand-coded support for [AVX-512](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions) SIMD instructions, which perform 8 double-precision floating-point operations simultaneously, so we will divide the BLAS performance by 8 for comparison purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(N, 2N.^3 ./ t * 1e-9, \"rs-\")\n",
    "plot(N, 2N.^3 ./ t0 * 1e-9 / 8, \"b.-\")\n",
    "ylabel(L\"gigaflops $2n^3/t$\")\n",
    "xlabel(L\"matrix size $n$\")\n",
    "legend([\"naive matmul\", \"BLAS matmul / 8\"], loc=\"center right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive matmul in C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be suspicious that the problem is simply that Julia is slow.   We can check this hypothesis by implementing the same algorithm in C, compiling it, and then calling it by Julia's built-in `ccall` instruction that makes it [easy to call C from Julia](http://docs.julialang.org/en/latest/manual/calling-c-and-fortran-code/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C implementation:\n",
    "Cmatmul = \"\"\"\n",
    "void Cmatmul(int m, int n, int p, double *C, double *A, double *B)\n",
    "{\n",
    "    int i, j, k;\n",
    "    for (i = 0; i < m; ++i)\n",
    "        for (j = 0; j < p; ++j) {\n",
    "            double c = 0.0;\n",
    "            for (k = 0; k < n; ++k)\n",
    "                c += A[i + m*k] * B[k + n*j];\n",
    "            C[i + m*j] = c;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "# compile to a shared library by piping Cmatmul to gcc:\n",
    "# (only works if you have gcc installed)\n",
    "const Clib = tempname()\n",
    "open(`gcc -fPIC -O3 -xc -shared -o $(Clib * \".\" * Libdl.dlext) -`, \"w\") do f\n",
    "    print(f, Cmatmul)\n",
    "end\n",
    "\n",
    "# define a Julia cmatmul! function that simply calls the C code in the shared library we compiled\n",
    "function cmatmul!(C, A, B)\n",
    "    m,n = size(A)\n",
    "    n,p = size(B)\n",
    "    size(C) == (m,p) || error(\"incorrect dimensions \", size(C), \" ≠ $m × $p\")\n",
    "    ccall((\"Cmatmul\", Clib), Void, (Cint, Cint, Cint, Ptr{Float64}, Ptr{Float64}, Ptr{Float64}),\n",
    "          m, n, p, C, A, B)\n",
    "    return C\n",
    "end\n",
    "cmatmul(A, B) = cmatmul!(Array(promote_type(eltype(A), eltype(B)),\n",
    "                               size(A,1), size(B,2)),\n",
    "                         A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correctness check:\n",
    "A = rand(5,6)\n",
    "B = rand(6,7)\n",
    "norm(cmatmul(A,B) - A * B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's benchmark it and print out the \"speedup\" compared to pure Julia.  We see that it is **about the same speed**.  Julia's main claim to fame is that it is a dynamic language (like Python or Matlab) that stays within a factor of 2 (usually better) of C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = Float64[]\n",
    "for n in N[N .<= 1000]\n",
    "    A = zeros(n,n)\n",
    "    B = zeros(n,n)\n",
    "    # preallocate output C so that allocation is not included in timing\n",
    "    C = zeros(n,n)\n",
    "    push!(tc, @elapsed cmatmul!(C,A,B))\n",
    "    println(\"finished n = $n: speedup of \", tc[end]/t[length(tc)])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache-oblivious matrix-multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step in the right direction, we'll implement a [cache-oblivious algorithm](https://en.wikipedia.org/wiki/Cache-oblivious_algorithm) for matrix multiplication: divide the matrices into four submatrices which are multiplied *recursively* until a sufficiently large base case is reached (large enough to amortize the recursion overhead).   This strategy erases the steep performance drop-off that occurs for large $n$ where the matrix goes out-of-cache, at the cost of ~25 lines of code rather than ~10 for the naive loops.\n",
    "\n",
    "(It still doesn't match the OpenBLAS performance because it fails to address the other two problems: unrolling and optimizing the base cases to optimize register utilization, and coding for SIMD instructions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function add_matmul_rec!(m,n,p, i0,j0,k0, C,A,B)\n",
    "    if m+n+p <= 64   # base case: naive matmult for sufficiently large matrices\n",
    "        for i = 1:m\n",
    "            for k = 1:p\n",
    "                c = zero(eltype(C))\n",
    "                for j = 1:n\n",
    "                    @inbounds c += A[i0+i,j0+j] * B[j0+j,k0+k]\n",
    "                end\n",
    "                @inbounds C[i0+i,k0+k] += c\n",
    "            end\n",
    "        end\n",
    "    else\n",
    "        m2 = m ÷ 2; n2 = n ÷ 2; p2 = p ÷ 2\n",
    "        add_matmul_rec!(m2, n2, p2, i0, j0, k0, C, A, B)\n",
    "        \n",
    "        add_matmul_rec!(m-m2, n2, p2, i0+m2, j0, k0, C, A, B)\n",
    "        add_matmul_rec!(m2, n-n2, p2, i0, j0+n2, k0, C, A, B)\n",
    "        add_matmul_rec!(m2, n2, p-p2, i0, j0, k0+p2, C, A, B)\n",
    "        \n",
    "        add_matmul_rec!(m-m2, n-n2, p2, i0+m2, j0+n2, k0, C, A, B)\n",
    "        add_matmul_rec!(m2, n-n2, p-p2, i0, j0+n2, k0+p2, C, A, B)\n",
    "        add_matmul_rec!(m-m2, n2, p-p2, i0+m2, j0, k0+p2, C, A, B)\n",
    "        \n",
    "        add_matmul_rec!(m-m2, n-n2, p-p2, i0+m2, j0+n2, k0+p2, C, A, B)\n",
    "    end\n",
    "    return C\n",
    "end\n",
    "\n",
    "function matmul_rec!(C, A, B)\n",
    "    m,n = size(A)\n",
    "    n,p = size(B)\n",
    "    size(C) == (m,p) || error(\"incorrect dimensions \", size(C), \" ≠ $m × $p\")\n",
    "    fill!(C, 0)\n",
    "    return add_matmul_rec!(m,n,p, 0,0,0, C,A,B)\n",
    "end\n",
    "\n",
    "\n",
    "matmul_rec(A, B) = matmul_rec!(Array(promote_type(eltype(A), eltype(B)),\n",
    "                                     size(A,1), size(B,2)),\n",
    "                               A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correctness check:\n",
    "A = rand(50,60)\n",
    "B = rand(60,70)\n",
    "norm(matmul_rec(A,B) - A * B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tco = Float64[]\n",
    "for n in N\n",
    "    A = zeros(n,n)\n",
    "    B = zeros(n,n)\n",
    "    # preallocate output C so that allocation is not included in timing\n",
    "    C = zeros(n,n)\n",
    "    push!(tco, @elapsed matmul_rec!(C,A,B))\n",
    "    println(\"finished n = $n: slowdown of \", tco[end]/t0[length(tco)])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(N, 2N.^3 ./ t * 1e-9, \"rs-\")\n",
    "plot(N, 2N.^3 ./ tco * 1e-9, \"ko-\")\n",
    "plot(N, 2N.^3 ./ t0 * 1e-9 / 8, \"b.-\")\n",
    "ylabel(L\"gigaflops $2n^3/t$\")\n",
    "xlabel(L\"matrix size $n$\")\n",
    "legend([\"naive matmul\", \"cache-oblivious\", \"BLAS matmul / 8\"], loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix addition\n",
    "\n",
    "Matrix addition is an interesting case because it has no data re-use, so there is no possible temporal locality, but depending on *what order* you use for the loops and *how matrices are stored* in memory, you may or may not get **spatial locality** that takes advantage of **cache lines**.\n",
    "\n",
    "Here let's implement matrix addition in two different ways.  As above, we'll use a pre-allocated output array so that our benchmark does not include the time for memory allocation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function matadd1!(C, A, B)\n",
    "    size(C) == size(A) == size(B) || throw(DimensionMismatchmatch())\n",
    "    m,n = size(A)\n",
    "    for i = 1:m\n",
    "        @simd for j = 1:n\n",
    "            @inbounds C[i,j] = A[i,j] + B[i,j]\n",
    "        end\n",
    "    end\n",
    "    return C\n",
    "end\n",
    "matadd1(A, B) = matadd1!(similar(A, promote_type(eltype(A), eltype(B))), A, B)\n",
    "\n",
    "function matadd2!(C, A, B)\n",
    "    size(C) == size(A) == size(B) || throw(DimensionMismatch())\n",
    "    m,n = size(A)\n",
    "    for j = 1:n\n",
    "        @simd for i = 1:m\n",
    "            @inbounds C[i,j] = A[i,j] + B[i,j]\n",
    "        end\n",
    "    end\n",
    "    return C\n",
    "end\n",
    "matadd2(A, B) = matadd2!(similar(A, promote_type(eltype(A), eltype(B))), A, B)\n",
    "\n",
    "A = rand(5,6)\n",
    "B = rand(5,6)\n",
    "A + B ≈ matadd1(A,B) ≈ matadd2(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Na = round(Int, logspace(1, log10(3000), 60))  # 60 sizes from 10 to 3000\n",
    "# alternatively, use N = 10:1000 to see some interesting patterns due to cache associativity etc.\n",
    "t1 = Float64[]\n",
    "t2 = Float64[]\n",
    "for n in Na\n",
    "    A = zeros(n,n)\n",
    "    B = zeros(n,n)\n",
    "    # preallocate output C so that allocation is not included in timing\n",
    "    C = zeros(n,n)\n",
    "    matadd1!(C,A,B) # add once just to make sure we are in cache if A and B are small\n",
    "    push!(t1, @elapsed matadd1!(C,A,B))\n",
    "    push!(t2, @elapsed matadd2!(C,A,B))\n",
    "    println(\"finished n = $n: ratio t1/t2 of \", t1[end]/t2[end])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(Na, Na.^2 ./ t1 * 1e-9, \"rs-\")\n",
    "plot(Na, Na.^2 ./ t2 * 1e-9, \"bo-\")\n",
    "xlabel(L\"matrix size $n$\")\n",
    "ylabel(L\"gigaflops $n^2/t$\")\n",
    "legend([\"by row\", \"by column\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(Na, t1 ./ t2, \"rs-\")\n",
    "xlabel(L\"matrix size $n$\")\n",
    "ylabel(\"by row time / by column time\")\n",
    "title(\"ratio of matrix-addition algorithms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that addition is about ***20 times slower*** if we add by rows rather than by columns.\n",
    "\n",
    "The reason for this is that **Julia stores matrices with consecutive columns**, which is known as **column-major storage** format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other examples: vectorized vs. devectorized operations\n",
    "\n",
    "Another example of the importance of spatial locality can be seen in vectorized operations.\n",
    "\n",
    "Consider the following function, which computes $f(x) = 2x^2 - 3x + 4$ *element-wise* for an array of $x$ values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_vec(x) = 2 * x.^2 - 3 * x + 4\n",
    "f_devec(X) = map(f_vec, X)\n",
    "\n",
    "X = rand(10^6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark f_vec($X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark f_devec($X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `f_devec` function is about **7 times faster** than the vectorized version, and allocates about **1/5 the memory**.   That is because `f_vec` is effectively equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f_vec_really(x)\n",
    "    tmp1 = x.^2\n",
    "    tmp2 = 2 * tmp1\n",
    "    tmp3 = 3 * x\n",
    "    tmp4 = tmp2 - tmp3\n",
    "    tmp5 = tmp4 + 4\n",
    "    return tmp5\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, each \"vectorized\" operation, like `x.^2`, while it is *individually fast*, does a *separate loop* over its data and *allocates a temporary array for the result*.  This is slow for three reasons:\n",
    "\n",
    "* It has poor temporal locality.\n",
    "* It is expensive to allocate all those temporary arrays.\n",
    "* The overhead of looping (incrementing/testing a counter, fetching elements `x[i]`, etcetera), is incurred multiple times rather than once.\n",
    "\n",
    "In Julia 0.6, there is a new syntax `2 .* x.^2 .- 3 .* x .+ 4` in which the compiler is guaranteed to \"fuse\" the operations into a single loop, allocating no temporary arrays.  In Julia 0.5, we can't do that, but it is sufficient to use `map(f, X)`, which does a single loop over `X`, calling `f(x)` for each element `x`, returning a new array."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.3",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
